HR Hiring Copilot â€” Architecture & Code Guide

This document explains the code structure, workflow, state model, prompts, tools, persistence, and extensibility.

1) High-Level Flow

Input -> LangGraph workflow -> (JD, Plan, Interview Kit) -> Deterministic assembly -> UI render + Tools

User Input
  |
  v
LangGraph (clarify -> extract -> jd -> plan -> kit -> assemble)
  |                                   |
  |                                   `-- app/llm.py (OpenAI/Ollama)
  v
AgentOutput (markdown + JSON)
  |
  `-- Saved to data/sessions/, analytics to data/analytics.jsonl

Clarify: optional LLM disambiguation
Extract: slots (roles, timeline_weeks, budget_range, location, seniority, employment_type, key_skills)
JD: one JD JSON per role, with startup tone and safe defaults
Plan: steps + owners + timeline
Kit: screening questions, evaluation guide, rubric, scorecard
Assemble: deterministic markdown + combined JSON (no LLM)

2) Repository Structure

app/
  agent.py        - entry that runs the LangGraph workflow
  comp.py         - compensation helpers (default budgets, comp notes)
  graph.py        - LangGraph nodes + parsing + assembly
  llm.py          - provider abstraction (OpenAI/Ollama) + connection tests
  memory.py       - sessions, analytics, preferences (file-based)
  prompts.py      - prompt templates used by nodes/tools
  schema.py       - Pydantic models (JD, Plan, Kit, Output, Slots)
  tools.py        - search sim, email writer, LinkedIn search, ATS check, exports
ui/
  streamlit_app.py- Streamlit UI (wizard, results, tools)

data/
  sessions/       - per-session outputs
  analytics.jsonl - usage log (JSON lines)

3) Workflow Details (app/graph.py)

State (TypedDict HiringGraphState):
  user_input, clarifications_asked, extraction, job_descriptions,
  hiring_plan, interview_kit, raw_markdown, raw_json, overrides,
  needs_clarification

Nodes:
- clarify_node: asks role clarifications via CLARIFY_ROLE_PROMPT; records questions/options; non-blocking.
- extract_node: builds SlotExtraction from overrides or SLOT_EXTRACT_PROMPT; normalizes roles/skills via ROLE_SKILL_NORMALIZE_PROMPT; fills defaults; coerces types; applies heuristics.
- jd_node: calls JD_GENERATION_PROMPT per role; enforces list sizes; injects min experience/qualification if provided; comp notes via comp.py.
- plan_node: calls PLAN_JSON_PROMPT; coerces steps/owners; fills owners.
- interview_kit_node: calls INTERVIEW_KIT_PROMPT; normalizes pairs; applies safe fallbacks.
- assemble_node: deterministic final markdown + JSON, no LLM.

Parsing: _loose_json_extract strips code fences, normalizes quotes, finds first valid JSON, removes trailing commas.

4) Prompts (app/prompts.py)
- CLARIFY_ROLE_PROMPT: { need_clarification, question, options, suggested_roles }
- SLOT_EXTRACT_PROMPT: structured slots from free text
- ROLE_SKILL_NORMALIZE_PROMPT: canonicalize roles, clean/expand skills
- JD_GENERATION_PROMPT: bounded JD JSON
- PLAN_JSON_PROMPT: plan JSON with steps/owners/timeline
- INTERVIEW_KIT_PROMPT: screening questions, evaluation pairs, rubric, scorecard
- SEARCH_RESULTS_PROMPT: synthesize search results

5) LLM Providers (app/llm.py)
- ask_llm routes to OpenAI or Ollama using env var LLM_PROVIDER
- OpenAI Chat Completions (openai>=1.31.0)
- Ollama HTTP /api/generate with configured model
- test_llm_connection: diagnostics for both providers

6) UI (ui/streamlit_app.py)
- Single path: hiring text -> Wizard
- Wizard captures timeline, budget, seniority, location, skills (+ advanced)
- Results tabs: Cards, Markdown, JSON, Refine
- Tools: Search, Email writer, LinkedIn search, ATS check, Job board shortcuts
- Sidebar: Provider toggle, analytics, session load/reset, preferences

7) Tools (app/tools.py)
- simulate_search: Google CSE (if keys) -> DuckDuckGo HTML -> LLM synthesis -> curated
- email_writer: context-aware outreach/scheduling drafts
- linkedin_candidate_search: boolean query + URL
- ats_resume_check: heuristic JD match score + notes
- checklist_export: Markdown checklist with owners and week estimates

8) Persistence (app/memory.py)
- save_state/load_state under data/sessions/
- analytics appended to data/analytics.jsonl
- preferences saved/loaded locally

9) Extensibility
- Add a node: implement in graph.py, register in create_hiring_graph, add edges, update UI if needed
- Add a tool: implement in tools.py; render in UI tools panel
- New provider: extend llm.py routing and add env options

10) Testing & Debugging
- Run: python -m pytest tests/ -v
- Smoke: python tests/test_graph.py
- Provider: python test_connection.py
- Inspect outputs: JSON tab or data/sessions/

11) Trade-offs
- JSON coercion over strict parsing (better local model tolerance)
- File storage (simple/local) vs DB (scale/multi-user)
- Salary is deterministic in assembly; not generated by LLM in final output

12) Request Lifecycle
User types: "Help me hire a data analyst"
 -> UI sets wizard intent
 -> run_agent_langgraph(user_input, overrides)
    -> clarify_node
    -> extract_node
    -> jd_node
    -> plan_node
    -> interview_kit_node
    -> assemble_node
 -> UI renders results + tools
 -> memory.save_state; analytics updated

End of document.
